{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming \n",
    "\n",
    "`Structured Streaming` — это масштабируемая и отказоустойчивая библиотека для потоковой обработки, построенный на базе `Spark SQL`. Основная идея - с потоковыми вычислениями можно работать так же, как и со статическими данными. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from typing import Iterator, List, Tuple\n",
    "\n",
    "import dbldatagen as dg\n",
    "import pyspark.sql.functions as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, session_window\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    " # Если переменная окружения  `JAVA_HOME` не установлена, то тут можно её указать.\n",
    "os.environ[\"JAVA_HOME\"] = \"/home/alex/java/jdk11\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем сессию `Spark`, как обычно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"structured\") \\\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", True) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем, создадим \"статический\" DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(\"row1\", 10), (\"row2\", 200)], [\"column1\", \"columns2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель исполнения:\n",
    "1. Входные данные поступают пачками (`mini-batch`) и добавляются к некоторому \"бесконечному\" DataFrame. Размер и частота появления `mini-batch` зависит от источника (генерируются \"по триггеру\").\n",
    "2. Пользователем описываются некоторые операции по преобразованию \"бесконечного DataFrame\", как в \"статическом\" Spark.\n",
    "3. В итоге получается \"результирующий DataFrame\", который является результатом работы и записывается во внешний источник (топик Kafka, консоль, файлы, etc)\n",
    "\n",
    "Создаем Streaming DataFrame, описывая процесс получения данных из какого-нибудь источника. Поддерживается 4 встроенных источника:\n",
    "- Kafka (`kafka`)\n",
    "- Файлы \n",
    "- Сеть (`socket`)\n",
    "- Генерация DataFrame вида `(timestamp TIMESTAMP, value LONG )`, для тестовых целей (`rate`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для Kafka нужно указать топик\n",
    "\n",
    "# df = spark \\\n",
    "#   .readStream \\\n",
    "#   .format(\"kafka\") \\\n",
    "#   .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "#   .option(\"subscribePattern\", \"topic*\") \\\n",
    "#   .option(\"startingOffsets\", \"earliest\") \\\n",
    "#   .load()\n",
    "\n",
    "\n",
    "# Будет создаваться 10 записей в секунду \n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", \"10\") \\\n",
    "    .load()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно запустить процесс обработки, дать поработать 10 секунд и остановить. Данные накапливаются в течение некоторого времени по триггеру в так называемый `mini-batch` и обрабатываются. Затем обновления добавляются в \"бесконечный\" DataFrame. Режим вывода может быть:\n",
    "- `update` - выводить только обновленные строки\n",
    "- `complete` - DataFrame полностью\n",
    "- `append` - новые строки\n",
    "\n",
    "Не все эти режимы доступны, зависит от применяемых операций обработки DataFrame. Результат будет выводиться в консоль. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека [dbldatagen](https://github.com/databrickslabs/dbldatagen) позволяет, для тестовых целей, генерировать DataFrame с заданной схемой и случайным содержимом. Создадим DataFrame с одной колонкой, в которой может быть одно из пяти заданных слов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# описываем данные, которые будут генерироваться\n",
    "ds = dg.DataGenerator(spark, name=\"Words\", rows=20, partitions=1) \\\n",
    "      .withColumn(\"word\", StringType(), values=[\"hello\", \"world\", \"ok\", \"no\", \"yes\"], weights=[1, 1, 2, 2, 2])\n",
    "\n",
    "# создаем Streaming DataFrame\n",
    "df = ds.build(withStreaming=True, options={'rowsPerSecond': 3})\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно описать преобразования (подсчет слов) и выводить текущую статистику в консоль"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.groupBy(\"word\").count()\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "\n",
    "time.sleep(30)\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проэмулируем получение данных от трех IoT-устройств"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dg.DataGenerator(spark, name=\"IOT\", rows=5000, partitions=1) \\\n",
    "      .withColumn(\"time\", \"timestamp\", expr=\"now()\") \\\n",
    "      .withColumn(\"sensor\", StringType(), values=[\"sensor_1\", \"sensor_2\", \"sensor_3\"]) \\\n",
    "      .withColumn(\"value\", \"integer\", minValue=0, maxValue=10, random=True)\n",
    "\n",
    "\n",
    "df = ds.build(withStreaming=True, options={'rowsPerSecond': 10})\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.build(withStreaming=True, options={'rowsPerSecond': 10})\n",
    "\n",
    "df = df.groupBy(\"sensor\").avg(\"value\")\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(30)\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно считать статистику по \"окнам\", которые образуются заданными временными интервалами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.build(withStreaming=True, options={'rowsPerSecond': 10})\n",
    "\n",
    "windowed_df = df\\\n",
    "    .groupBy(\n",
    "        window(df.time, \"10 seconds\"),\n",
    "        df.sensor\n",
    "    ).avg(\"value\")\n",
    "\n",
    "\n",
    "query = windowed_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(60)\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Окна могут быть накладываться друг на друга или быть привязаны к какому-то полю события (\"сессионные\")\n",
    ".\n",
    "![](https://spark.apache.org/docs/latest/img/structured-streaming-time-window-types.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пересекающиеся - 10 секунд каждые 5.\n",
    "windowed_df = df\\\n",
    "   .groupBy(\n",
    "        window(df.time, \"10 seconds\", \"5 seconds\"),\n",
    "        df.sensor) \\\n",
    "    .avg(\"value\")\n",
    "\n",
    "# Разный размер окна в зависимости от датчика\n",
    "sw = session_window(df.time, \\\n",
    "    F.when(df.sensor == \"sensor_3\", \"5 seconds\") \\\n",
    "     .when(df.sensor == \"sensor_2\", \"10 seconds\") \\\n",
    "     .otherwise(\"5 seconds\"))\n",
    "\n",
    "windowed_df = df\\\n",
    "    .groupBy(\n",
    "        sw,\n",
    "        df.sensor) \\\n",
    "    .avg(\"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Событие может быть создано намного раньше времени физической обработки Spark'ом. Например, это может быть связано с высокой нагрузкой или проблемой с сетью. Обработка такого события приведет к изменению исторических данных. Для того, чтобы избежать подобного, можно воспользоваться \"watermark\", указав максимальное время между значением поля времени и временем обработки события. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "windowed_df = df\\\n",
    "    .withWatermark(\"time\", \"15 seconds\") \\\n",
    "    .groupBy(\n",
    "        window(df.time, \"10 seconds\"),\n",
    "        df.sensor\n",
    "    ).avg(\"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Spark Streaming` поддерживает операции вида `join` между двумя датафреймами. Причем датафрейм может быть статическим или динамическим. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.build(withStreaming=True, options={'rowsPerSecond': 10})\n",
    "\n",
    "descr_df = spark.createDataFrame([(\"sensor_1\", \"Sensor #1\"), (\"sensor_2\", \"Sensor #2\"), (\"sensor_3\", \"Sensor #3\")], [\"sensor\", \"description\"])\n",
    "\n",
    "res_df = df.join(descr_df, \"sensor\")\n",
    "\n",
    "query = res_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(30)\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.build(withStreaming=True, options={'rowsPerSecond': 10})\n",
    "\n",
    "windows_df_5 = df \\\n",
    "    .withWatermark(\"time\", \"30 seconds\") \\\n",
    "    .groupBy(\n",
    "        window(\"time\", \"5 seconds\"),\n",
    "        \"sensor\") \\\n",
    "    .agg(F.avg(\"value\").alias(\"value\")) \\\n",
    "    .select(F.window_time(\"window\").alias(\"time\"), \"value\", \"sensor\")\n",
    "\n",
    "\n",
    "windows_df_10 = df \\\n",
    "    .withWatermark(\"time\", \"20 seconds\") \\\n",
    "    .groupBy(\n",
    "        window(\"time\", \"10 seconds\"),\n",
    "        \"sensor\") \\\n",
    "    .agg(F.avg(\"value\").alias(\"value\")) \\\n",
    "    .select(F.window_time(\"window\").alias(\"time\"), \"value\", \"sensor\")    \n",
    " \n",
    " \n",
    "res_df = windows_df_5.alias(\"df_5\").join(\n",
    "    windows_df_10.alias(\"df_10\"),\n",
    "    F.expr(\"\"\"\n",
    "        df_5.sensor = df_10.sensor  AND\n",
    "        df_5.time >= df_10.time AND\n",
    "        df_5.time <= df_10.time + interval 40 seconds\n",
    "        \"\"\")\n",
    ")\n",
    "\n",
    "query = res_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(50)\n",
    "\n",
    "query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_sp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
