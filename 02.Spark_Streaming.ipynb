{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming \n",
    "\n",
    "`Structured Streaming` — это масштабируемая и отказоустойчивая библиотека для потоковой обработки, построенный на базе `Spark SQL`. Основная идея - с потоковыми вычислениями можно работать так же, как и со статическими данными. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from typing import Iterator, List, Tuple\n",
    "\n",
    "import dbldatagen as dg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    " # Если переменная окружения  `JAVA_HOME` не установлена, то тут можно её указать.\n",
    "os.environ[\"JAVA_HOME\"] = \"/home/alex/java/jdk11\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем сессию `Spark`, как обычно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 17:33:20 WARN Utils: Your hostname, burg resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "23/10/22 17:33:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/22 17:33:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"structured\") \\\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", True) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем, создадим \"статический\" DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(\"row1\", 10), (\"row2\", 200)], [\"column1\", \"columns2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель исполнения:\n",
    "1. Входные данные поступают пачками (`mini-batch`) и добавляются к некоторому \"бесконечному\" DataFrame. Размер и частота появления `mini-batch` зависит от источника (генерируются \"по триггеру\").\n",
    "2. Пользователем описываются некоторые операции по преобразованию \"бесконечного DataFrame\", как в \"статическом\" Spark.\n",
    "3. В итоге получается \"результирующий DataFrame\", который является результатом работы и записывается во внешний источник (топик Kafka, консоль, файлы, etc)\n",
    "\n",
    "Создаем Streaming DataFrame, описывая процесс получения данных из какого-нибудь источника. Поддерживается 4 встроенных источника:\n",
    "- Kafka (`kafka`)\n",
    "- Файлы \n",
    "- Сеть (`socket`)\n",
    "- Генерация DataFrame вида `(timestamp TIMESTAMP, value LONG )`, для тестовых целей (`rate`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Для Kafka нужно указать топик\n",
    "\n",
    "# df = spark \\\n",
    "#   .readStream \\\n",
    "#   .format(\"kafka\") \\\n",
    "#   .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "#   .option(\"subscribePattern\", \"topic*\") \\\n",
    "#   .option(\"startingOffsets\", \"earliest\") \\\n",
    "#   .load()\n",
    "\n",
    "\n",
    "# Будет создаваться 10 записей в секунду \n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", \"10\") \\\n",
    "    .load()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно запустить процесс обработки, дать поработать 10 секунд и остановить. Данные накапливаются в течение некоторого времени по триггеру в так называемый `mini-batch` и обрабатываются. Затем обновления добавляются в \"бесконечный\" DataFrame. Режим вывода может быть:\n",
    "- `update` - выводить только обновленные строки\n",
    "- `complete` - DataFrame полностью\n",
    "- `append` - новые строки\n",
    "\n",
    "Не все эти режимы доступны, зависит от применяемых операций обработки DataFrame. Результат будет выводиться в консоль. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+-----+\n",
      "|timestamp|value|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2023-10-22 17:51:28.436|0    |\n",
      "|2023-10-22 17:51:28.836|4    |\n",
      "|2023-10-22 17:51:29.236|8    |\n",
      "|2023-10-22 17:51:29.636|12   |\n",
      "|2023-10-22 17:51:30.036|16   |\n",
      "|2023-10-22 17:51:28.536|1    |\n",
      "|2023-10-22 17:51:28.936|5    |\n",
      "|2023-10-22 17:51:29.336|9    |\n",
      "|2023-10-22 17:51:29.736|13   |\n",
      "|2023-10-22 17:51:30.136|17   |\n",
      "|2023-10-22 17:51:28.636|2    |\n",
      "|2023-10-22 17:51:29.036|6    |\n",
      "|2023-10-22 17:51:29.436|10   |\n",
      "|2023-10-22 17:51:29.836|14   |\n",
      "|2023-10-22 17:51:30.236|18   |\n",
      "|2023-10-22 17:51:28.736|3    |\n",
      "|2023-10-22 17:51:29.136|7    |\n",
      "|2023-10-22 17:51:29.536|11   |\n",
      "|2023-10-22 17:51:29.936|15   |\n",
      "|2023-10-22 17:51:30.336|19   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2023-10-22 17:51:30.436|20   |\n",
      "|2023-10-22 17:51:30.836|24   |\n",
      "|2023-10-22 17:51:31.236|28   |\n",
      "|2023-10-22 17:51:31.636|32   |\n",
      "|2023-10-22 17:51:32.036|36   |\n",
      "|2023-10-22 17:51:30.536|21   |\n",
      "|2023-10-22 17:51:30.936|25   |\n",
      "|2023-10-22 17:51:31.336|29   |\n",
      "|2023-10-22 17:51:31.736|33   |\n",
      "|2023-10-22 17:51:32.136|37   |\n",
      "|2023-10-22 17:51:30.636|22   |\n",
      "|2023-10-22 17:51:31.036|26   |\n",
      "|2023-10-22 17:51:31.436|30   |\n",
      "|2023-10-22 17:51:31.836|34   |\n",
      "|2023-10-22 17:51:32.236|38   |\n",
      "|2023-10-22 17:51:30.736|23   |\n",
      "|2023-10-22 17:51:31.136|27   |\n",
      "|2023-10-22 17:51:31.536|31   |\n",
      "|2023-10-22 17:51:31.936|35   |\n",
      "|2023-10-22 17:51:32.336|39   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2023-10-22 17:51:32.436|40   |\n",
      "|2023-10-22 17:51:32.836|44   |\n",
      "|2023-10-22 17:51:33.236|48   |\n",
      "|2023-10-22 17:51:32.536|41   |\n",
      "|2023-10-22 17:51:32.936|45   |\n",
      "|2023-10-22 17:51:33.336|49   |\n",
      "|2023-10-22 17:51:32.636|42   |\n",
      "|2023-10-22 17:51:33.036|46   |\n",
      "|2023-10-22 17:51:32.736|43   |\n",
      "|2023-10-22 17:51:33.136|47   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2023-10-22 17:51:33.436|50   |\n",
      "|2023-10-22 17:51:33.836|54   |\n",
      "|2023-10-22 17:51:34.236|58   |\n",
      "|2023-10-22 17:51:33.536|51   |\n",
      "|2023-10-22 17:51:33.936|55   |\n",
      "|2023-10-22 17:51:34.336|59   |\n",
      "|2023-10-22 17:51:33.636|52   |\n",
      "|2023-10-22 17:51:34.036|56   |\n",
      "|2023-10-22 17:51:33.736|53   |\n",
      "|2023-10-22 17:51:34.136|57   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2023-10-22 17:51:34.436|60   |\n",
      "|2023-10-22 17:51:34.836|64   |\n",
      "|2023-10-22 17:51:35.236|68   |\n",
      "|2023-10-22 17:51:34.536|61   |\n",
      "|2023-10-22 17:51:34.936|65   |\n",
      "|2023-10-22 17:51:35.336|69   |\n",
      "|2023-10-22 17:51:34.636|62   |\n",
      "|2023-10-22 17:51:35.036|66   |\n",
      "|2023-10-22 17:51:34.736|63   |\n",
      "|2023-10-22 17:51:35.136|67   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2023-10-22 17:51:35.436|70   |\n",
      "|2023-10-22 17:51:35.836|74   |\n",
      "|2023-10-22 17:51:36.236|78   |\n",
      "|2023-10-22 17:51:35.536|71   |\n",
      "|2023-10-22 17:51:35.936|75   |\n",
      "|2023-10-22 17:51:36.336|79   |\n",
      "|2023-10-22 17:51:35.636|72   |\n",
      "|2023-10-22 17:51:36.036|76   |\n",
      "|2023-10-22 17:51:35.736|73   |\n",
      "|2023-10-22 17:51:36.136|77   |\n",
      "+-----------------------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2023-10-22 17:51:36.436|80   |\n",
      "|2023-10-22 17:51:36.836|84   |\n",
      "|2023-10-22 17:51:37.236|88   |\n",
      "|2023-10-22 17:51:36.536|81   |\n",
      "|2023-10-22 17:51:36.936|85   |\n",
      "|2023-10-22 17:51:37.336|89   |\n",
      "|2023-10-22 17:51:36.636|82   |\n",
      "|2023-10-22 17:51:37.036|86   |\n",
      "|2023-10-22 17:51:36.736|83   |\n",
      "|2023-10-22 17:51:37.136|87   |\n",
      "+-----------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека [dbldatagen](https://github.com/databrickslabs/dbldatagen) позволяет, для тестовых целей, генерировать DataFrame с заданной схемой и случайным содержимом. Создадим DataFrame с одной колонкой, в которой может быть одно из пяти заданных слов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# описываем данные, которые будут генерироваться\n",
    "ds = dg.DataGenerator(spark, name=\"Words\", rows=20, partitions=1) \\\n",
    "      .withColumn(\"word\", StringType(), values=[\"hello\", \"world\", \"ok\", \"no\", \"yes\"], weights=[1, 1, 2, 2, 2])\n",
    "\n",
    "# создаем Streaming DataFrame\n",
    "df = ds.build(withStreaming=True, options={'rowsPerSecond': 3})\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно описать преобразования (подсчет слов) и выводить текущую статистику в консоль"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|word |count|\n",
      "+-----+-----+\n",
      "|hello|5    |\n",
      "|ok   |8    |\n",
      "|no   |8    |\n",
      "|world|4    |\n",
      "|yes  |8    |\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|word |count|\n",
      "+-----+-----+\n",
      "|hello|7    |\n",
      "|ok   |13   |\n",
      "|no   |12   |\n",
      "|world|7    |\n",
      "|yes  |12   |\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|word |count|\n",
      "+-----+-----+\n",
      "|hello|9    |\n",
      "|ok   |18   |\n",
      "|no   |17   |\n",
      "|world|9    |\n",
      "|yes  |16   |\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 17:58:01 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=false]] is aborting.\n",
      "23/10/22 17:58:01 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 4, writer: ConsoleWriter[numRows=20, truncate=false]] aborted.\n",
      "[Stage 17:===>                                                   (13 + 4) / 200]\r"
     ]
    }
   ],
   "source": [
    "\n",
    "df = df.groupBy(\"word\").count()\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "\n",
    "time.sleep(30)\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проэмулируем получение данных от трех IoT-устройств"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: timestamp (nullable = false)\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- value: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds = dg.DataGenerator(spark, name=\"IOT\", rows=1000, partitions=1) \\\n",
    "      .withColumn(\"time\", \"timestamp\", expr=\"now()\") \\\n",
    "      .withColumn(\"sensor\", StringType(), values=[\"sensor_1\", \"sensor_2\", \"sensor_3\"]) \\\n",
    "      .withColumn(\"value\", \"integer\", minValue=0, maxValue=10, random=True)\n",
    "\n",
    "\n",
    "df = ds.build(withStreaming=True, options={'rowsPerSecond': 10})\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+----------+\n",
      "|sensor|avg(value)|\n",
      "+------+----------+\n",
      "+------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------+----------+\n",
      "|sensor  |avg(value)|\n",
      "+--------+----------+\n",
      "|sensor_1|5.0       |\n",
      "|sensor_2|5.85      |\n",
      "|sensor_3|5.5       |\n",
      "+--------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------+----------+\n",
      "|sensor  |avg(value)|\n",
      "+--------+----------+\n",
      "|sensor_1|4.875     |\n",
      "|sensor_2|5.55      |\n",
      "|sensor_3|5.7       |\n",
      "+--------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+--------+-----------------+\n",
      "|sensor  |avg(value)       |\n",
      "+--------+-----------------+\n",
      "|sensor_1|4.701754385964913|\n",
      "|sensor_2|5.228070175438597|\n",
      "|sensor_3|5.642857142857143|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+--------+-----------------+\n",
      "|sensor  |avg(value)       |\n",
      "+--------+-----------------+\n",
      "|sensor_1|4.972972972972973|\n",
      "|sensor_2|5.36986301369863 |\n",
      "|sensor_3|5.712328767123288|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 18:01:29 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: ConsoleWriter[numRows=20, truncate=false]] is aborting.\n",
      "23/10/22 18:01:29 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 5, writer: ConsoleWriter[numRows=20, truncate=false]] aborted.\n",
      "23/10/22 18:01:29 ERROR ShuffleBlockFetcherIterator: Error occurred while fetching local blocks, null\n",
      "23/10/22 18:01:29 ERROR Utils: Aborting task=====>              (145 + 2) / 200]\n",
      "java.lang.IllegalStateException: Error committing version 6 into HDFSStateStore[id=(op=0,part=145),dir=file:/tmp/temporary-7c70df23-09d1-4199-87d4-05419f922538/state/0/145]\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:148)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.$anonfun$close$3(statefulOperators.scala:597)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:552)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:179)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:467)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.close(statefulOperators.scala:597)\n",
      "\tat org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:66)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$1(WriteToDataSourceV2Exec.scala:441)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:486)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:425)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:388)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-7c70df23-09d1-4199-87d4-05419f922538/state/0/145 does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1614)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:206)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:790)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:496)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:720)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1036)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:372)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:154)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:450)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:320)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:141)\n",
      "\t... 31 more\n",
      "23/10/22 18:01:29 ERROR DataWritingSparkTask: Aborting commit for partition 145 (task 2283, attempt 0, stage 33.0)\n",
      "23/10/22 18:01:29 ERROR DataWritingSparkTask: Aborted commit for partition 145 (task 2283, attempt 0, stage 33.0)\n"
     ]
    }
   ],
   "source": [
    "df = ds.build(withStreaming=True, options={'rowsPerSecond': 10})\n",
    "\n",
    "df = df.groupBy(\"sensor\").avg(\"value\")\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(30)\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно считать статистику по \"окнам\", которые образуются заданными временными интервалами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+------+----------+\n",
      "|window|sensor|avg(value)|\n",
      "+------+------+----------+\n",
      "+------+------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------------------------------------------+--------+-----------------+\n",
      "|window                                    |sensor  |avg(value)       |\n",
      "+------------------------------------------+--------+-----------------+\n",
      "|{2023-10-22 18:08:50, 2023-10-22 18:09:00}|sensor_1|4.882352941176471|\n",
      "|{2023-10-22 18:08:50, 2023-10-22 18:09:00}|sensor_3|4.9375           |\n",
      "|{2023-10-22 18:08:50, 2023-10-22 18:09:00}|sensor_2|5.470588235294118|\n",
      "+------------------------------------------+--------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------------------------------------------+--------+------------------+\n",
      "|window                                    |sensor  |avg(value)        |\n",
      "+------------------------------------------+--------+------------------+\n",
      "|{2023-10-22 18:09:00, 2023-10-22 18:09:10}|sensor_1|4.882352941176471 |\n",
      "|{2023-10-22 18:09:00, 2023-10-22 18:09:10}|sensor_3|4.142857142857143 |\n",
      "|{2023-10-22 18:09:00, 2023-10-22 18:09:10}|sensor_3|4.9375            |\n",
      "|{2023-10-22 18:09:00, 2023-10-22 18:09:10}|sensor_2|5.923076923076923 |\n",
      "|{2023-10-22 18:09:00, 2023-10-22 18:09:10}|sensor_1|5.3076923076923075|\n",
      "|{2023-10-22 18:09:00, 2023-10-22 18:09:10}|sensor_2|5.470588235294118 |\n",
      "+------------------------------------------+--------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+------------------------------------------+--------+-----------------+\n",
      "|window                                    |sensor  |avg(value)       |\n",
      "+------------------------------------------+--------+-----------------+\n",
      "|{2023-10-22 18:09:00, 2023-10-22 18:09:10}|sensor_1|4.882352941176471|\n",
      "|{2023-10-22 18:09:00, 2023-10-22 18:09:10}|sensor_3|4.62962962962963 |\n",
      "|{2023-10-22 18:09:00, 2023-10-22 18:09:10}|sensor_3|4.9375           |\n",
      "|{2023-10-22 18:09:00, 2023-10-22 18:09:10}|sensor_2|5.076923076923077|\n",
      "|{2023-10-22 18:09:00, 2023-10-22 18:09:10}|sensor_1|4.518518518518518|\n",
      "|{2023-10-22 18:09:00, 2023-10-22 18:09:10}|sensor_2|5.470588235294118|\n",
      "+------------------------------------------+--------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+------------------------------------------+--------+-----------------+\n",
      "|window                                    |sensor  |avg(value)       |\n",
      "+------------------------------------------+--------+-----------------+\n",
      "|{2023-10-22 18:09:10, 2023-10-22 18:09:20}|sensor_3|4.615384615384615|\n",
      "|{2023-10-22 18:09:10, 2023-10-22 18:09:20}|sensor_1|4.882352941176471|\n",
      "|{2023-10-22 18:09:10, 2023-10-22 18:09:20}|sensor_3|4.62962962962963 |\n",
      "|{2023-10-22 18:09:10, 2023-10-22 18:09:20}|sensor_3|4.9375           |\n",
      "|{2023-10-22 18:09:10, 2023-10-22 18:09:20}|sensor_2|5.076923076923077|\n",
      "|{2023-10-22 18:09:10, 2023-10-22 18:09:20}|sensor_1|3.923076923076923|\n",
      "|{2023-10-22 18:09:10, 2023-10-22 18:09:20}|sensor_1|4.518518518518518|\n",
      "|{2023-10-22 18:09:10, 2023-10-22 18:09:20}|sensor_2|5.571428571428571|\n",
      "|{2023-10-22 18:09:10, 2023-10-22 18:09:20}|sensor_2|5.470588235294118|\n",
      "+------------------------------------------+--------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:==================================================>   (188 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+------------------------------------------+--------+-----------------+\n",
      "|window                                    |sensor  |avg(value)       |\n",
      "+------------------------------------------+--------+-----------------+\n",
      "|{2023-10-22 18:09:10, 2023-10-22 18:09:20}|sensor_3|4.333333333333333|\n",
      "|{2023-10-22 18:09:10, 2023-10-22 18:09:20}|sensor_1|4.882352941176471|\n",
      "|{2023-10-22 18:09:10, 2023-10-22 18:09:20}|sensor_3|4.62962962962963 |\n",
      "|{2023-10-22 18:09:10, 2023-10-22 18:09:20}|sensor_3|4.9375           |\n",
      "|{2023-10-22 18:09:10, 2023-10-22 18:09:20}|sensor_2|5.076923076923077|\n",
      "|{2023-10-22 18:09:10, 2023-10-22 18:09:20}|sensor_1|4.666666666666667|\n",
      "|{2023-10-22 18:09:10, 2023-10-22 18:09:20}|sensor_1|4.518518518518518|\n",
      "|{2023-10-22 18:09:10, 2023-10-22 18:09:20}|sensor_2|5.433333333333334|\n",
      "|{2023-10-22 18:09:10, 2023-10-22 18:09:20}|sensor_2|5.470588235294118|\n",
      "+------------------------------------------+--------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+------------------------------------------+--------+-----------------+\n",
      "|window                                    |sensor  |avg(value)       |\n",
      "+------------------------------------------+--------+-----------------+\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_3|4.333333333333333|\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_1|4.882352941176471|\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_3|4.62962962962963 |\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_3|4.9375           |\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_2|5.076923076923077|\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_2|6.117647058823529|\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_3|4.588235294117647|\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_1|4.666666666666667|\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_1|4.518518518518518|\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_2|5.433333333333334|\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_1|3.5625           |\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_2|5.470588235294118|\n",
      "+------------------------------------------+--------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+------------------------------------------+--------+-----------------+\n",
      "|window                                    |sensor  |avg(value)       |\n",
      "+------------------------------------------+--------+-----------------+\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_3|4.333333333333333|\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_1|4.882352941176471|\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_3|4.62962962962963 |\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_3|4.9375           |\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_2|5.076923076923077|\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_2|5.147058823529412|\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_3|4.515151515151516|\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_1|4.666666666666667|\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_1|4.518518518518518|\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_2|5.433333333333334|\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_1|4.393939393939394|\n",
      "|{2023-10-22 18:09:20, 2023-10-22 18:09:30}|sensor_2|5.470588235294118|\n",
      "+------------------------------------------+--------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/22 18:09:32 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: ConsoleWriter[numRows=20, truncate=false]] is aborting.\n",
      "23/10/22 18:09:32 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 8, writer: ConsoleWriter[numRows=20, truncate=false]] aborted.\n"
     ]
    }
   ],
   "source": [
    "df = ds.build(withStreaming=True, options={'rowsPerSecond': 10})\n",
    "\n",
    "windowed_df = df \\\n",
    ".groupBy(\n",
    "    window(df.time, \"10 seconds\"),\n",
    "    df.sensor\n",
    ").avg(\"value\")\n",
    "\n",
    "\n",
    "query = windowed_df \\\n",
    "    .writeStream \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(40)\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_sp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
