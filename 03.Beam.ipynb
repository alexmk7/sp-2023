{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Beam\n",
    "\n",
    "Фреймворк для потоковой обработки данных, который может работать поверх различных движков (`runners`): `Google Cloud Dataflow`, `Apache Spark`,  `Direct Runner`, ... \n",
    "\n",
    "`Pipeline` - конструируемый пользователь граф, в котором определяется последовательность действий над данными\n",
    "\n",
    "`PCollection` - набор неизменяемых данных или поток неизменяемых данных, который преобразуется в рамках `Pipeline`\n",
    "\n",
    "`PTransform` - операция преобразования данных, шаг в `Pipeline`. Применяется к набору `PCollection` и в результате получается набор `PCollection`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На базовом уровне программа на `Beam` выглядит так:\n",
    "\n",
    "- создать объект `Pipeline` и задать необходимые параметры \n",
    "- создать начальный набор данных `PCollection`, используя внешние хранилища или данные хранящиеся в памяти\n",
    "- применять `PTransforms` к `PCollection`. `PTransforms` могут изменять, фильтровать, группировать, анализировать или иным образом обрабатывать элементы в `PCollection`. Преобразование создает новую `PCollection` без изменения исходной `PCollection`. Процессы преобразований не обязательно линейны, можно строить граф обработки произвольной сложности.\n",
    "- запустить `Pipeline` используя какой-нибудь `Runner`. Для демонстрации в здесь будет использоваться `Direct Runner`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим простой `Pipeline`. Весь процесс можно описать в виде простого `DSL`, который использует оператор `|`:\n",
    "\n",
    "```bash\n",
    "[Output PCollection] = [Input PCollection] | [Transform]\n",
    "```\n",
    "\n",
    "- `PCollection` в памяти из 10 чисел\n",
    "- отфильтруем четные числа\n",
    "- выведем их на экран"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import apache_beam as beam\n",
    "from typing import List, Tuple, Dict, Iterable, Iterator\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    # p - Pipeline\n",
    "    p | beam.Create(range(1, 11)) \\\n",
    "      | beam.Filter(lambda num: num % 2 == 0) \\\n",
    "      | beam.Map(print) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трансформации можно аннотировать с помощью оператора `<<`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as p:\n",
    "    p | \"Create\" >> beam.Create(range(1, 11)) \\\n",
    "      | \"Filter\" >> beam.Filter(lambda num: num % 2 == 0) \\\n",
    "      | \"Print\" >> beam.Map(print) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParDo\n",
    "\n",
    "Базовое, определенное пользователем преобразование, можно создать с помощью наследования от класса `DoFn`. При этом на код, который реализует преобразования, накладываются определенные ограничения:\n",
    "\n",
    "- они должны быть сериализуемы\n",
    "- идемпотентны\n",
    "- потокобезопасны\n",
    "\n",
    "`ParDo` — это базовый `PTransform` для параллельной обработки данных.  `ParDo` рассматривает каждый элемент во входной `PCollection`, выполняет некоторую функцию обработки (пользовательский код, `DoFn`) над этим элементом и выдает ноль, один или несколько элементов в выходную `PCollection`.\n",
    "\n",
    "ParDo может быть применен для множества распространенных операций обработки данных, в том числе:\n",
    "\n",
    "- для фильтрации набора данных (аналог функции `filter` в стандартной библиотеки `Python` или других фреймворках для потоков обработки)\n",
    "- для преобразование каждого элемента в наборе данных (аналог `map`)\n",
    "- для выполнения вычислений и генерации новых данных (`flatMap`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home\n",
      "4\n",
      "sweet\n",
      "5\n",
      "home\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "class BreakIntoWordsDoFn(beam.DoFn):\n",
    "    def process(self, element: str) -> Iterator[str]:\n",
    "        return element.split()\n",
    "        #yield from element.split()\n",
    "\n",
    "    \n",
    "class LenDoFn(beam.DoFn):\n",
    "    def process(self, word: str) -> Iterator[int]:\n",
    "        return [len(word)]\n",
    "    \n",
    "class PrintFn(beam.DoFn):\n",
    "    def process(self, entity) -> None:\n",
    "        print(entity)\n",
    "        return None\n",
    "        \n",
    "    \n",
    "with beam.Pipeline() as p:\n",
    "    words = p | \"Text lines\"  >> beam.Create([\"home\", \"sweet home\"]) \\\n",
    "              | beam.ParDo(BreakIntoWordsDoFn()) \n",
    "    \n",
    "    out = words | \"Print words\" >> beam.ParDo(PrintFn())\n",
    "\n",
    "    out_len = words | beam.ParDo(LenDoFn()) \\\n",
    "                    | \"Print lengths\" >> beam.ParDo(PrintFn())                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   Иногда, в случае простых преобразований, проще использовать простые lambda-функции и преобразования:\n",
    "   - `Filter`\n",
    "   - `Map`\n",
    "   - `FlatMap`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "home\n",
      "4\n",
      "sweet\n",
      "5\n",
      "home\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as p:\n",
    "    words = p | \"Text lines\"  >> beam.Create([\"home \", \"sweet home\"]) \\\n",
    "              | beam.FlatMap(lambda text: text.split()) \n",
    "    \n",
    "    out = words | \"Print words\" >> beam.Map(print)\n",
    "\n",
    "    out_len = words | beam.Map(lambda x: len(x)) \\\n",
    "                    | \"Print lengths\" >> beam.Map(print)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupByKey\n",
    "`GroupByKey` — это преобразование для обработки коллекций вида пар ключ/значение. Это параллельная операция свертки. Входные данные для `GroupByKey` — это коллекция пар ключ/значение, представляющая словарь, который может содержать несколько пар с одинаковым ключом, но разными значениями.  `GroupByKey` можно использовать для обработки значений, связанных с каждым уникальным ключом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('привет', [5, 4])\n",
      "('мир', [1])\n"
     ]
    }
   ],
   "source": [
    "data = [(\"привет\", 5), (\"мир\", 1), (\"привет\", 4)]\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    input = p | beam.Create(data) \\\n",
    "              | beam.GroupByKey() \\\n",
    "              | beam.Map(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoGroupByKey\n",
    "\n",
    "`CoGroupByKey` позволяет группировать по ключам сразу несколько `PCollection`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('привет', {'docs': [[1, 2, 3], [4, 5, 6]], 'freqs': [8]})\n",
      "('мир', {'docs': [[1, 3]], 'freqs': [10]})\n"
     ]
    }
   ],
   "source": [
    "word_documents_list = [\n",
    "    ('привет', [1, 2, 3]),\n",
    "    ('привет', [4, 5, 6]),\n",
    "    ('мир', [1, 3]),\n",
    "]\n",
    "\n",
    "word_freq_list = [\n",
    "    ('привет', 8),\n",
    "    ('мир', 10),\n",
    "]\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    word_documents = p | \"Create wd\" >> beam.Create(word_documents_list)\n",
    "    word_freq = p | \"Create freq\" >> beam.Create(word_freq_list)\n",
    "\n",
    "    results = {'docs': word_documents, 'freqs': word_freq} \\\n",
    "                | beam.CoGroupByKey() \\\n",
    "                | beam.Map(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine\n",
    "\n",
    "Позволяет свертывать значения, аналог сверток."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with beam.Pipeline() as p:\n",
    "    p | beam.Create(range(1, 5)) \\\n",
    "      | beam.CombineGlobally(sum) \\\n",
    "      | beam.Map(print)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно реализовать собственный класс для неассоциативных операций:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MeanFn(beam.CombineFn):\n",
    "    def create_accumulator(self):\n",
    "        return (0.0, 0)\n",
    "\n",
    "    def add_input(self, state: Tuple[float, int], input: float) -> Tuple[float, int]:\n",
    "        (sum, count) = state\n",
    "        return sum + input, count + 1\n",
    "\n",
    "    def merge_accumulators(self, accumulators: List[Tuple[float, int]]) -> Tuple[float, int]:\n",
    "        sums, counts = zip(*accumulators)\n",
    "        return sum(sums), sum(counts)\n",
    "\n",
    "    def extract_output(self, state: Tuple[float, int]) -> float:\n",
    "        (sum, count) = state\n",
    "        return sum / count if count else float('NaN')\n",
    "    \n",
    "with beam.Pipeline() as p:\n",
    "    p | beam.Create(range(1, 6)) \\\n",
    "      | beam.CombineGlobally(MeanFn()) \\\n",
    "      | beam.Map(print)     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно делать свертку по ключу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('привет', 9.0)\n",
      "('мир', 10.0)\n"
     ]
    }
   ],
   "source": [
    "word_freq_list = [\n",
    "    ('привет', 8),\n",
    "    ('мир', 10),\n",
    "    ('привет', 10),\n",
    "]\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    p | beam.Create(word_freq_list) \\\n",
    "      | beam.CombinePerKey(MeanFn()) \\\n",
    "      | beam.Map(print)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten\n",
    "\n",
    "Позволяет объединить несколько `PCollection` в одно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as p:\n",
    "    p_col1 = p | \"PCol 1\" >> beam.Create(range(1, 5))\n",
    "    p_col2 = p | \"PCol 2\" >> beam.Create(range(10, 15))\n",
    "\n",
    "    (p_col1, p_col2) \\\n",
    "      | beam.Flatten() \\\n",
    "      | beam.Map(print)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition\n",
    "Позволяет разделить один `PCollection` на несколько"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3\n",
      "6\n",
      "9\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as p:\n",
    "    parts = p | beam.Create(range(0, 15)) \\\n",
    "              | beam.Partition(lambda x, t: x % t, 3) \n",
    "       \n",
    "    parts[0] | beam.Map(print)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Состояния\n",
    "\n",
    "Можно поддерживать состояния для подсчета статистики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "from apache_beam.transforms.userstate import ReadModifyWriteStateSpec\n",
    "\n",
    "class StateDoFn(beam.DoFn):\n",
    "  STATE_SPEC = ReadModifyWriteStateSpec('num_elements', beam.coders.VarIntCoder())\n",
    "\n",
    "  def process(self, element: Tuple[str, int], state=beam.DoFn.StateParam(STATE_SPEC)):\n",
    "    current_value = state.read() or 0\n",
    "    state.write(current_value + element[1])\n",
    "    return [state.read()]\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    parts = p | beam.Create([(\"привет\", 1), (\"мир\", 2), (\"привет\", 7)]) \\\n",
    "              | beam.ParDo(StateDoFn()) \\\n",
    "              | beam.Map(print)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "def filter_func(el: int, avg_param: int) -> bool:\n",
    "    return el < avg_param\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    inp = p | beam.Create(range(15)) \n",
    "            \n",
    "    avg = inp | beam.CombineGlobally(MeanFn()) \n",
    "\n",
    "    inp | beam.Filter(filter_func, avg_param=beam.pvalue.AsSingleton(avg)) \\\n",
    "        | beam.Map(print)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поддержка пакетных операций\n",
    "\n",
    "Для оптимизации кода можно применять обработку данных с помощью `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n",
      "9\n",
      "16\n",
      "25\n",
      "36\n",
      "49\n",
      "64\n",
      "81\n",
      "100\n",
      "121\n",
      "144\n",
      "169\n",
      "196\n",
      "225\n",
      "256\n",
      "289\n",
      "324\n",
      "361\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Square(beam.DoFn):\n",
    "    def process_batch(self, batch: np.ndarray) -> Iterator[np.ndarray]:\n",
    "        yield batch ** 2\n",
    "\n",
    "    def infer_output_type(self, input_element_type):\n",
    "        return input_element_type\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    p | beam.Create(range(20)).with_output_types(np.int64) \\\n",
    "      | beam.ParDo(Square()) \\\n",
    "      | beam.Map(print)\n",
    "    \n",
    "# with beam.Pipeline() as p:\n",
    "#     p | beam.Create(range(20)) \\\n",
    "#       | beam.Map(lambda x: x ** 2) \\\n",
    "#       | beam.Map(print)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.152263636363635\n",
      "70.50923636363636\n",
      "56.536663636363635\n"
     ]
    }
   ],
   "source": [
    "from apache_beam.dataframe.io import read_csv\n",
    "from apache_beam.dataframe.convert import to_pcollection, to_dataframe\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    currencies = p | read_csv(\"data/currencies.csv\")\n",
    "\n",
    "    agg = currencies.groupby(\"cur\").val.mean()\n",
    "    \n",
    "    to_pcollection(agg) | beam.Map(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Естественно можно использовать SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.transforms.sql import SqlTransform\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    currencies = p | read_csv(\"data/currencies.csv\") \\\n",
    "                   | SqlTransform(\"\"\"SELECT cur, AVG(val) FROM PCOLLECTION GROUP BY cur\"\"\") \\\n",
    "                   | beam.Map(print)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно создавать DataFrame на лету"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BeamSchema_ba49671c_0c24_432e_8347_1d475a7bf9b5(count=2)\n",
      "BeamSchema_ba49671c_0c24_432e_8347_1d475a7bf9b5(count=8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with beam.Pipeline() as p:\n",
    "    parts = p | beam.Create([(\"привет\", 1), (\"мир\", 2), (\"привет\", 7)]) \\\n",
    "              | beam.Map(lambda pair: beam.Row(word=pair[0], count=pair[1])) \n",
    "    df = to_dataframe(parts) \n",
    "    agg = df.groupby(\"word\").sum()\n",
    "    to_pcollection(agg) | beam.Map(print)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потоковые данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('hello', 3)\n",
      "('world', 1)\n",
      "('york', 1)\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import time\n",
    "\n",
    "from apache_beam.transforms import window\n",
    "\n",
    "class PrintFn(beam.DoFn):\n",
    "    def process(\n",
    "        self,\n",
    "        element: Tuple[str, int],\n",
    "        timestamp=beam.DoFn.TimestampParam,\n",
    "        window=beam.DoFn.WindowParam\n",
    "    ):  \n",
    "        print(element)\n",
    "        yield element\n",
    "\n",
    "class ChangeTimestamp(beam.DoFn):\n",
    "    def process(\n",
    "        self,\n",
    "        element: str,\n",
    "    ): \n",
    "        timestamp = int(time.time())\n",
    "        yield beam.window.TimestampedValue(element, timestamp)\n",
    "\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    messages =(p | beam.Create([b\"hello world\", b\"hello hello york\"]) #beam.io.ReadFromPubSub(subscription=\"some_channel\").with_output_types(bytes) \\\n",
    "                 | \"Decode\" >> beam.Map(lambda x: x.decode('utf-8')) \n",
    "                 | \"Add time\" >> beam.ParDo(ChangeTimestamp())\n",
    "                 | \"Split\"  >> beam.FlatMap(lambda line: re.findall(r\"\\w+\", line)) \n",
    "                 | beam.Map(lambda x: (x, 1)) \n",
    "                 | beam.WindowInto(window.FixedWindows(15, 0)) \n",
    "                 | beam.GroupByKey() \n",
    "                 | beam.Map(lambda kv: (kv[0], sum(kv[1]))) \n",
    "                 | beam.ParDo(PrintFn())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример с подсчетом tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('человек', (0, 0.6437751649736402))\n",
      "('лев', (0, 0.04462871026284196))\n",
      "('лев', (1, 0.07438118377140325))\n",
      "('лев', (2, 0.11157177565710488))\n",
      "('лев', (4, 0.07438118377140325))\n",
      "('орел', (0, 0.10216512475319815))\n",
      "('орел', (1, 0.1702752079219969))\n",
      "('орел', (4, 0.1702752079219969))\n",
      "('черепаха', (0, 0.18325814637483104))\n",
      "('черепаха', (2, 0.22907268296853878))\n",
      "('вол', (1, 0.5364793041447))\n",
      "('кошка', (2, 0.22907268296853878))\n",
      "('кошка', (3, 0.3054302439580517))\n",
      "('жучка', (3, 0.5364793041447))\n",
      "('мышка', (3, 0.5364793041447))\n",
      "('грифон', (4, 0.5364793041447))\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "docs = [\n",
    "    \"человек лев орел черепаха человек\", \n",
    "    \"лев вол орел\",\n",
    "    \"лев черепаха лев кошка\",\n",
    "    \"жучка кошка мышка\",\n",
    "    \"лев орел грифон\"\n",
    "]\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    id_to_content = p | beam.Create(enumerate(docs))\n",
    "\n",
    "    total_documents = (\n",
    "        id_to_content\n",
    "        | beam.Keys()\n",
    "        | beam.Distinct()\n",
    "        | beam.combiners.Count.Globally()\n",
    "    )\n",
    "\n",
    "    id_to_words = (\n",
    "        id_to_content\n",
    "        | \"SplitWords\" >> beam.FlatMap(lambda id_and_doc: [(id_and_doc[0], word) for word in re.findall(r\"\\w+\", id_and_doc[1])])\n",
    "    )\n",
    "\n",
    "    word_to_doc_count = (\n",
    "        id_to_words\n",
    "        | \"GetUniqueWordsPerDoc\" >> beam.Distinct()\n",
    "        | \"GetWords\" >> beam.Values()\n",
    "        | \"CountDocsPerWord\" >> beam.combiners.Count.PerElement()\n",
    "    )\n",
    "\n",
    "    id_to_word_total = (\n",
    "        id_to_words\n",
    "        | \"GetIds\" >> beam.Keys()\n",
    "        | \"CountWordsInDoc\" >> beam.combiners.Count.PerElement())\n",
    "\n",
    "\n",
    "    id_and_word_to_count = (\n",
    "        id_to_words\n",
    "        | \"CountWord-DocPairs\" >> beam.combiners.Count.PerElement())\n",
    "\n",
    "    id_to_word_and_count = (\n",
    "        id_and_word_to_count\n",
    "        | \"ShiftKeys\" >> beam.Map(lambda id_word_count: (id_word_count[0][0], (id_word_count[0][1], id_word_count[1])))\n",
    "    )\n",
    "\n",
    "    id_to_word_and_count_and_total = ({\n",
    "         \"word totals\": id_to_word_total, \n",
    "         \"word counts\": id_to_word_and_count\n",
    "         } | \"CoGroupByUri\" >> beam.CoGroupByKey())\n",
    "\n",
    "    \n",
    "    def compute_term_frequency(id_count_and_total: Tuple[int, Dict]) -> Iterator[Tuple[str, Tuple[int, float]]]:\n",
    "        (id, count_and_total) = id_count_and_total\n",
    "        word_and_count = count_and_total[\"word counts\"]\n",
    "\n",
    "        [word_total] = count_and_total[\"word totals\"]\n",
    "        for word, count in word_and_count:\n",
    "            yield word, (id, float(count) / word_total)\n",
    "\n",
    "    word_to_id_and_tf = (\n",
    "        id_to_word_and_count_and_total\n",
    "        | \"ComputeTermFrequencies\" >> beam.FlatMap(compute_term_frequency))\n",
    "\n",
    "    word_to_df = (\n",
    "        word_to_doc_count\n",
    "        | \"ComputeDocFrequencies\" >> beam.Map(lambda word_and_count, total: (word_and_count[0], float(word_and_count[1]) / total), \n",
    "                                              beam.pvalue.AsSingleton(total_documents))\n",
    "    )\n",
    "\n",
    "    word_to_id_and_tf_and_df = ({\n",
    "        \"tf\": word_to_id_and_tf, \"df\": word_to_df\n",
    "    } | \"CoGroupWordsByTf-df\" >> beam.CoGroupByKey())\n",
    "\n",
    "    def compute_tf_idf(word_tf_and_df: Tuple[str, Dict]) -> Iterator[Tuple[str, Tuple[int, float]]]:\n",
    "      (word, tf_and_df) = word_tf_and_df\n",
    "      [docf] = tf_and_df[\"df\"]\n",
    "      for id, tf in tf_and_df[\"tf\"]:\n",
    "        yield word, (id, tf * math.log(1 / docf))\n",
    "\n",
    "    word_to_id_and_tfidf = (\n",
    "        word_to_id_and_tf_and_df\n",
    "        | \"ComputeTf-idf\" >> beam.FlatMap(compute_tf_idf)\n",
    "    )\n",
    "\n",
    "    word_to_id_and_tfidf | beam.Map(print)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_sp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
